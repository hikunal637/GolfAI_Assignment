{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports (can be imported individually later too, if needed)\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mediapipe as mp\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749221499.472110 13029533 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M2\n",
      "W0000 00:00:1749221499.570467 13272788 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749221499.585886 13272793 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1350 images for Address\n",
      "Processing: Finish\n",
      "Processed 1350 images for Finish\n",
      "Processing: Idle\n",
      "Processed 1350 images for Idle\n",
      "Processing: Impact\n",
      "Processed 1350 images for Impact\n",
      "Processing: Mid-Backswing\n",
      "Processed 1350 images for Mid-Backswing\n",
      "Processing: Mid-Downswing\n",
      "Processed 1350 images for Mid-Downswing\n",
      "Processing: Mid-Follow-Through\n",
      "Processed 1350 images for Mid-Follow-Through\n",
      "Processing: Toe-up\n",
      "Processed 1350 images for Toe-up\n",
      "Processing: Top\n",
      "Processed 1350 images for Top\n"
     ]
    }
   ],
   "source": [
    "'''Processes a dataset of images to extract pose landmarks using MediaPipe Pose. It reads images from the folder Swing_events, extracts pose features, and stores them.\n",
    "It uses MediaPipe to extract pose landmarks from each image and builds a dataset of pose feature vectors (X) with corresponding labels (y).\n",
    "This code assumes that the dataset is organized in subfolders, where each subfolder corresponds to a different class (swing phases). \n",
    "I have used a maximum of 1350 images per class here.'''\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True)\n",
    "\n",
    "def extract_pose_from_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(img_rgb)\n",
    "    if result.pose_landmarks:\n",
    "        return np.array([[lm.x, lm.y, lm.z] for lm in result.pose_landmarks.landmark]).flatten()\n",
    "    return None\n",
    "\n",
    "def process_dataset(base_folder, max_per_class=1350):\n",
    "    X, y = [], []\n",
    "    for label, folder in enumerate(sorted(os.listdir(base_folder))):\n",
    "        folder_path = os.path.join(base_folder, folder)\n",
    "        if not os.path.isdir(folder_path): continue\n",
    "\n",
    "        print(f\"Processing: {folder}\")\n",
    "        count = 0\n",
    "        for fname in sorted(os.listdir(folder_path)):\n",
    "            if count >= max_per_class:\n",
    "                break\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            features = extract_pose_from_image(fpath)\n",
    "            if features is not None:\n",
    "                X.append(features)\n",
    "                y.append(folder)  # Using folder name as label\n",
    "                count += 1\n",
    "        print(f\"Processed {count} images for {folder}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = process_dataset('Swing_events', max_per_class=1350)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Address       0.84      0.83      0.84       270\n",
      "            Finish       0.93      0.92      0.93       270\n",
      "              Idle       0.93      0.95      0.94       270\n",
      "            Impact       0.77      0.82      0.79       270\n",
      "     Mid-Backswing       0.74      0.77      0.75       270\n",
      "     Mid-Downswing       0.74      0.70      0.72       270\n",
      "Mid-Follow-Through       0.87      0.87      0.87       270\n",
      "            Toe-up       0.92      0.86      0.89       270\n",
      "               Top       0.90      0.92      0.91       270\n",
      "\n",
      "          accuracy                           0.85      2430\n",
      "         macro avg       0.85      0.85      0.85      2430\n",
      "      weighted avg       0.85      0.85      0.85      2430\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pose_classifier.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Trains a RandomForestClassifier on pose landmarks to classify golf swing phases.\n",
    "It evaluates the model using precision, recall, and F1-score, and saves the trained model to disk as pose_classifier.pkl.'''\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "import joblib\n",
    "joblib.dump(clf, 'pose_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Classifies each frame of a video using trained pose classifier.\n",
    "Extracts pose landmarks with MediaPipe, predicts the swing phase for each frame,\n",
    "and saves the results (frame_id, predicted_label) to a text file.'''\n",
    "\n",
    "\n",
    "def classify_video(video_path, model, pose, output_txt='classified_frames.txt'):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    results = []\n",
    "\n",
    "    frame_id = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = pose.process(img_rgb)\n",
    "\n",
    "        if result.pose_landmarks:\n",
    "            pose_vec = np.array([[lm.x, lm.y, lm.z] for lm in result.pose_landmarks.landmark]).flatten().reshape(1, -1)\n",
    "            pred = model.predict(pose_vec)[0]\n",
    "        else:\n",
    "            pred = 'NoPose'\n",
    "\n",
    "        results.append((frame_id, pred))\n",
    "        frame_id += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with open(output_txt, 'w') as f:\n",
    "        for frame_id, label in results:\n",
    "            f.write(f\"{frame_id},{label}\\n\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749223325.892914 13029533 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1749223325.972705 13291538 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749223325.988769 13291538 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "#TO LABEL FRAMES IN A VIDEO AND SAVE THE OUTPUT VIDEO WITH LABELS\n",
    "\n",
    "'''Loads the model and has the function to annotate each frame of a video with predicted golf swing phase.\n",
    "The labeled video is saved to disk with the text overlay of predicted class.'''\n",
    "\n",
    "# Load model\n",
    "model = joblib.load(\"pose_classifier.pkl\")\n",
    "\n",
    "# Initialize MediaPipe pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False)\n",
    "\n",
    "# Define function to label and save output video\n",
    "def label_and_save_video(input_video_path, output_video_path, model, pose_model):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Set up video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_id = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = pose_model.process(img_rgb)\n",
    "\n",
    "        if result.pose_landmarks:\n",
    "            pose_vec = np.array([[lm.x, lm.y, lm.z] for lm in result.pose_landmarks.landmark]).flatten().reshape(1, -1)\n",
    "            pred_label = model.predict(pose_vec)[0]\n",
    "        else:\n",
    "            pred_label = \"NoPose\"\n",
    "\n",
    "        # Put text label on the frame\n",
    "        cv2.putText(frame, f\"{pred_label}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Write frame to output\n",
    "        out.write(frame)\n",
    "        frame_id += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Saved annotated video to {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotated video to latest7.mp4\n"
     ]
    }
   ],
   "source": [
    "'''Actual code to execute the labeling and saving of the video, using functions defined in previous cell.'''\n",
    "\n",
    "label_and_save_video(\n",
    "    input_video_path='sample_vid1.mp4',\n",
    "    output_video_path='latest7.mp4',\n",
    "    model=model,\n",
    "    pose_model=pose\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749223794.969482 13029533 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M2\n",
      "W0000 00:00:1749223795.041648 13297038 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749223795.053636 13297038 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved swing_0.mp4 from frame 1484 to 1536\n",
      "Saved swing_1.mp4 from frame 1541 to 1608\n"
     ]
    }
   ],
   "source": [
    "## MAIN CODE TO EXTRACT SWING CLIPS FROM A VIDEO\n",
    "\n",
    "\n",
    "video_path = \"sample_vid3.mp4\"  # update this path\n",
    "output_dir='swing_clips4'       # directly where you want to save the extracted clips\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mediapipe as mp\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Load pose classifier\n",
    "model = joblib.load(\"pose_classifier.pkl\")\n",
    "\n",
    "# Initialize MediaPipe pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False)\n",
    "\n",
    "# Define swing classes\n",
    "swing_classes = [\n",
    "    \"Address\", \"Takeaway\", \"Mid-Backswing\", \"Top\",\n",
    "    \"Mid-Downswing\", \"Impact\", \"Follow-through\", \"Finish\"\n",
    "]\n",
    "\n",
    "# Step 1: Classify each frame\n",
    "def classify_video(video_path, model, pose):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    results = []\n",
    "    frame_id = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = pose.process(img_rgb)\n",
    "\n",
    "        if result.pose_landmarks:\n",
    "            pose_vec = np.array([[lm.x, lm.y, lm.z] for lm in result.pose_landmarks.landmark]).flatten().reshape(1, -1)\n",
    "            pred = model.predict(pose_vec)[0]\n",
    "        else:\n",
    "            pred = 'NoPose'\n",
    "\n",
    "        results.append((frame_id, pred))\n",
    "        frame_id += 1\n",
    "\n",
    "    cap.release()\n",
    "    return results\n",
    "\n",
    "# Step 2: Smooth predictions with 4-frame window\n",
    "def smooth_predictions(results, window_size=4):\n",
    "    smoothed = []\n",
    "    labels = [label for _, label in results]\n",
    "    for i in range(len(labels)):\n",
    "        window = labels[max(0, i - window_size//2): min(len(labels), i + window_size//2)]\n",
    "        most_common = Counter(window).most_common(1)[0][0]\n",
    "        smoothed.append((results[i][0], most_common))\n",
    "    return smoothed\n",
    "\n",
    "# Step 3: Detect swings starting with \"Address\" and ending with \"Finish\"\n",
    "def extract_address_to_finish_segments(preds, min_swing_length=20):\n",
    "    swing_segments = []\n",
    "    start = None\n",
    "    temp_segment = []\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        frame_id, label = preds[i]\n",
    "\n",
    "        if label == \"Address\":\n",
    "            # Start new swing or restart if another Address comes before Finish\n",
    "            start = frame_id\n",
    "            temp_segment = [(frame_id, label)]\n",
    "\n",
    "        elif start is not None:\n",
    "            temp_segment.append((frame_id, label))\n",
    "\n",
    "            if label == \"Finish\":\n",
    "                # Extend to last consecutive \"Finish\"\n",
    "                j = i\n",
    "                while j + 1 < len(preds) and preds[j + 1][1] == \"Finish\":\n",
    "                    j += 1\n",
    "                    temp_segment.append(preds[j])\n",
    "\n",
    "                end = preds[j][0]\n",
    "\n",
    "                # Apply filters: length + required labels\n",
    "                labels_in_segment = [lbl for _, lbl in temp_segment]\n",
    "                if (\n",
    "                    end - start >= min_swing_length and\n",
    "                    \"Mid-Backswing\" in labels_in_segment and\n",
    "                    \"Mid-Downswing\" in labels_in_segment\n",
    "                ):\n",
    "                    swing_segments.append((start, end))\n",
    "\n",
    "                start = None\n",
    "                temp_segment = []\n",
    "\n",
    "    return swing_segments\n",
    "\n",
    "# Step 4: Save swing clips\n",
    "def save_swing_clips(video_path, segments, output_dir='swing_clips'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    for idx, (start, end) in enumerate(segments):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "        out = cv2.VideoWriter(os.path.join(output_dir, f'swing_{idx}.mp4'), fourcc, fps, (width, height))\n",
    "\n",
    "        for f in range(start, end + 1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            out.write(frame)\n",
    "        out.release()\n",
    "        print(f\"Saved swing_{idx}.mp4 from frame {start} to {end}\")\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Run all steps\n",
    "frame_results = classify_video(video_path, model, pose)\n",
    "smoothed = smooth_predictions(frame_results, window_size=4)\n",
    "segments = extract_address_to_finish_segments(smoothed, min_swing_length=20)\n",
    "save_swing_clips(video_path, segments, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
